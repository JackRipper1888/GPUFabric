#include <stdio.h>
#include <stdlib.h>
#include <string.h>

typedef int LlamaToken;

// C interface function declarations
extern int gpuf_init(void);
extern void* gpuf_load_model(const char* path);
extern void* gpuf_create_context(void* model);
extern void gpuf_cleanup(void);

extern int gpuf_generate_with_sampling(
    const void* model,
    void* ctx, 
    const char* prompt,
    int max_tokens,
    float temperature,
    int top_k,
    float top_p,
    float repeat_penalty,
    char* output,
    int output_len,
    LlamaToken* token_buffer,
    int token_buffer_size
);

int main(int argc, char* argv[]) {
    printf("ğŸ§ª Android Inference Test - OPTIMIZED PARAMETERS\n");
    printf("===============================================\n\n");
    
    if (argc != 2) {
        printf("Usage: %s \"prompt\"\n", argv[0]);
        printf("Example: %s \"Hello\"\n", argv[0]);
        printf("Example: %s \"What is your name?\"\n", argv[0]);
        return 1;
    }
    
    const char* prompt = argv[1];
    printf("ğŸ“ Testprompt: \"%s\"\n\n", prompt);
    
    // Initialize[ç³»][ç»Ÿ]
    printf("ğŸ”§ Initializing GPUFabric SDK...\n");
    if (!gpuf_init()) {
        printf("âŒ System initialization failed\n");
        return 1;
    }
    printf("âœ… System initialization successful\n\n");
    
    // LoadModel
    printf("ğŸ“¦ Loading SmolVLM-500M model...\n");
    const char* model_path = "/data/local/tmp/SmolVLM-500M-Instruct-Q8_0.gguf";
    void* model = gpuf_load_model(model_path);
    if (!model) {
        printf("âŒ Model loading failed: %s\n", model_path);
        gpuf_cleanup();
        return 1;
    }
    printf("âœ… Model loaded successfully\n\n");
    
    // createbuildupdowntext
    printf("ğŸ¯ Creating inference context...\n");
    void* ctx = gpuf_create_context(model);
    if (!ctx) {
        printf("âŒ Context creation failed\n");
        gpuf_cleanup();
        return 1;
    }
    printf("âœ… Context created successfully\n\n");
    
    // Generatetextscript - useuseexcellent-izeParameters
    printf("ğŸš€ Starting AI inference...\n");
    printf("âš™ï¸  excellent-izeParameters: Temperature=0.8, Top-K=40, Top-P=0.9, Repeat=1.1\n\n");
    
    char output[1024] = {0};
    LlamaToken token_buffer[32];
    
    int result = gpuf_generate_with_sampling(
        model, ctx, prompt,
        40,      // increaseaddto 40 tokens
        0.8f,    // provide[é«˜][æ¸©]degreeto 0.8
        40,      // increaseadd Top-K to 40
        0.9f,    // provide[é«˜] Top-P to 0.9
        1.1f,    // [æ·»]add[é‡]complexpenalty[ç½š] 1.1
        output, sizeof(output) - 1,
        token_buffer, 32
    );
    
    printf("ğŸ“Š Inference Results:\n");
    printf("=============\n");
    
    if (result > 0) {
        printf("âœ… Generation successful!\n");
        printf("ğŸ“ Output: \"%s\"\n", output);
        printf("ğŸ“Š Length: %d tokens\n\n", result);
        
        // partanalyzeOutput[è´¨][é‡]
        printf("ğŸ” Output[è´¨][é‡]partanalyze:\n");
        if (strlen(output) > 10) {
            printf("âœ… Generatecompletedhavemeaningmeaning[çš„]internalcontain\n");
        } else {
            printf("âš ï¸  internalcontain[è¿‡][çŸ­]\n");
        }
        
        if (strstr(output, " ") && strstr(output, ".")) {
            printf("âœ… packagecontaincompletewhole[çš„]sentencechild[ç»“]structure\n");
        } else {
            printf("âš ï¸  sentencechild[ç»“]structurenotcompletewhole\n");
        }
        
        if (strstr(output, prompt)) {
            printf("âš ï¸  packagecontain[é‡]complex[çš„]prompt\n");
        } else {
            printf("âœ… [æ²¡]have[é‡]complexprompt\n");
        }
    } else {
        printf("âŒ GenerateFailed: Errorgeneration[ç ] %d\n", result);
    }
    
    // Cleanup[èµ„][æº]
    printf("\nğŸ§¹ Cleaning up resources...\n");
    gpuf_cleanup();
    
    printf("\nğŸ‰ Android AI pushmanageTestCompletedï¼\n");
    printf("=====================================\n");
    return 0;
}
